{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('metis': conda)"
  },
  "interpreter": {
   "hash": "9ff5619d0b4d23b346007d49da5f2b2df0f81ce2bd1fd38999567ed2108bbf81"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "#Import necessary packages\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import  NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#Import created pipeline class\n",
    "from nlp_pipeline import nlp_pipeline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "#Load in pickled objects\n",
    "news_df = pickle.load(open('news_df','rb'))\n",
    "word2vec_model = pickle.load(open('word2vec_model','rb'))\n",
    "doc_vectors = pickle.load(open('doc_vectors','rb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#Create dataframe with article sentiments\n",
    "news_titles = news_df['title'].to_list()\n",
    "\n",
    "sent_score_dict = {}\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for title in news_titles:\n",
    "    vs = analyzer.polarity_scores(title)\n",
    "    sent_score_dict[title] = vs['compound']\n",
    "\n",
    "sent_score_df = pd.DataFrame.from_dict(sent_score_dict, orient='index').reset_index()\n",
    "sent_score_df.columns = ['article','vader_compound']\n",
    "\n",
    "#Add general catergories\n",
    "def compound_sorter(score):\n",
    "    if score > 0:\n",
    "        return 1\n",
    "    elif score == 0:\n",
    "        return 0\n",
    "    elif score < 0:\n",
    "        return -1\n",
    "\n",
    "sent_score_df['sentiment'] = sent_score_df['vader_compound'].apply(compound_sorter)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#Function to clean/tokenize words\n",
    "def clean_and_tokenize(document):\n",
    "    token_list = word_tokenize(document)\n",
    "    cleaned_words = []\n",
    "    for word in token_list:\n",
    "        low_word = re.sub('[\\d\\W]', '', word).lower()\n",
    "        if low_word:\n",
    "            cleaned_words.append(low_word)\n",
    "    return cleaned_words\n",
    "\n",
    "#Function to create document vectors\n",
    "not_in_model = []\n",
    "\n",
    "def vectorize_document(cleaned_title_words, model):\n",
    "    list_of_word_vectors = []\n",
    "    for token in cleaned_title_words:\n",
    "        if token in model.wv.vocab:\n",
    "            list_of_word_vectors.append(model[token])\n",
    "        else:\n",
    "            not_in_model.append(token)\n",
    "    doc_vector = np.mean(list_of_word_vectors, axis=0)\n",
    "    return doc_vector"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "#Recommender function\n",
    "\n",
    "def article_opposite(input_query, model, doc_vectors):\n",
    "\n",
    "    #Find sentiment\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    new_sentiment = compound_sorter(analyzer.polarity_scores(input_query)['compound']) #-1 negative, +1 positive\n",
    "\n",
    "    #Find topic\n",
    "    new_topic = vectorize_document(clean_and_tokenize(input_query), model).reshape(1, -1)\n",
    "    potential_article_return_list = pairwise_distances(new_topic,doc_vectors,metric='cosine').argsort()\n",
    "    articles_to_return = []\n",
    "    already_added_1 = False\n",
    "    already_added_2 = False\n",
    "    if new_sentiment == 0:\n",
    "        for article in potential_article_return_list[0]:\n",
    "            article_sentiment = sent_score_df.iloc[article]['sentiment']\n",
    "            if (article_sentiment == 1) and (already_added_1 == False):\n",
    "                articles_to_return.append(article)\n",
    "                already_added_1 = True\n",
    "            elif (article_sentiment == -1) and (already_added_2 == False):\n",
    "                articles_to_return.append(article)\n",
    "                already_added_2 = True\n",
    "            elif (already_added_1 == True) and (already_added_2 == True):\n",
    "                return articles_to_return\n",
    "    elif new_sentiment == 1:\n",
    "        for article in potential_article_return_list[0]:\n",
    "            article_sentiment = sent_score_df.iloc[article]['sentiment']\n",
    "            if (article_sentiment == 0) and (already_added_1 == False):\n",
    "                articles_to_return.append(article)\n",
    "                already_added_1 = True\n",
    "            elif (article_sentiment == -1) and (already_added_2 == False):\n",
    "                articles_to_return.append(article)\n",
    "                already_added_2 = True\n",
    "            elif (already_added_1 == True) and (already_added_2 == True):\n",
    "                return articles_to_return\n",
    "    elif new_sentiment == -1:\n",
    "        for article in potential_article_return_list[0]:\n",
    "            article_sentiment = sent_score_df.iloc[article]['sentiment']\n",
    "            if (article_sentiment == 0) and (already_added_1 == False):\n",
    "                articles_to_return.append(article)\n",
    "                already_added_1 = True\n",
    "            elif (article_sentiment == 1) and (already_added_2 == False):\n",
    "                articles_to_return.append(article)\n",
    "                already_added_2 = True\n",
    "            elif (already_added_1 == True) and (already_added_2 == True):\n",
    "                return articles_to_return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "#Test run\n",
    "\n",
    "recommended_articles = article_opposite('Wives of alleged Haiti assassins left in the dark, desperate for word or to repatriate bodies',word2vec_model, doc_vectors)\n",
    "article_1 = sent_score_df.iloc[recommended_articles[0]]['article']\n",
    "article_2 = sent_score_df.iloc[recommended_articles[1]]['article']\n",
    "print('Article 1: {art1}'.format(art1 = article_1))\n",
    "print('Article 2: {art2}'.format(art2 = article_2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Article 1: How supply-chain innovation can bolster U.S. security\n",
      "Article 2: U.S. sending FBI, DHS law enforcement to Haiti\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-19-71cc1e0f50ee>:17: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if token in model.wv.vocab:\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "#Pickle objects for easier transition to web app\n",
    "\n",
    "pickle.dump(sent_score_df, open('sent_score_df','wb')) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}